{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for file handling\n",
    "import pathlib # for reading files\n",
    "import pandas as pd # for reading csv files\n",
    "import numpy as np # for numerical operations\n",
    "from PIL import Image # for opening images\n",
    "import shutil # for moving files\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "import tensorflow as tf # for deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, Precision, Recall\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Num GPUs Available:  1\n",
      "Physical Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Physical Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data using Keras utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\cats_hrs_training_2024_06_06\\\\training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 cats but recently a fourth cat is showing up. Since we have only few imaes of this cat let's ignore this for now and train only for our own 3 cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#Larger batches also provide a more accurate estimate of the gradient, but they require more memory.\n",
    "\n",
    "img_height = 240 # Using the actual sizes takes too long for training. Grösse halbiert.\n",
    "img_width = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, if label_mode is not specified, it is set to \"int\", which means the function automatically assigns an integer label to each class. Classes are usually indexed according to the alphanumeric order of the folder names. Therefore, each image is labeled with the integer representing its class index.\n",
    "\n",
    "'h' -> 0\n",
    "\n",
    "'o' -> 1\n",
    "\n",
    "'r' -> 2\n",
    "\n",
    "'s' -> 3\n",
    "\n",
    "SparseCategoricalCrossentropy the appropriate choice for the loss function because it expects the labels to be provided as integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the class names in the `class_names` attribute on these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image can be represented as a 3-D tensor. For a color image, the dimensions might correspond to height, width, and color channels (RGB). The `image_batch` is a tensor of the shape `(32, 480, 640, 3)`. This is a batch of 32 images of shape `480x640x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n",
    "\n",
    "You can call `.numpy()` on either of these tensors to convert them to a `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n",
    "\n",
    "- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "- `Dataset.prefetch` overlaps data preprocessing and model execution while training.\n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the *Prefetching* section of the [Better performance with the tf.data API](../../guide/data_performance.ipynb) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small.\n",
    "\n",
    "Here, you will standardize values to be in the `[0, 1]` range by using `tf.keras.layers.Rescaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not neccessary if it is included as the first layer in the model\n",
    "# normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use this layer. You can apply it to the dataset by calling `Dataset.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can include the layer inside your model definition to simplify deployment. Use the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `softmax` function is particularly suited for multi-class classification problems where each class is mutually exclusive.\n",
    "\n",
    "When using `softmax`, set `from_logits=False`\n",
    "\n",
    "When not using `softmax` (last layer outputs raw scores), set `from_logits=True` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) model consists of three convolution blocks (`tf.keras.layers.Conv2D`) with a max pooling layer (`tf.keras.layers.MaxPooling2D`) in each of them. There's a fully-connected layer (`tf.keras.layers.Dense`) with 128 units on top of it that is activated by a ReLU activation function (`'relu'`). This model has not been tuned in any way—the goal is to show you the mechanics using the datasets you just created. To learn more about image classification, visit the [Image classification](../images/classification.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "  layers.Dense(num_classes, activation='softmax') # Using softmax for probability output\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rescaling layer only handles the normalization of pixel values. It does not perform resizing. The input_shape parameter in the Rescaling layer simply specifies the expected input dimensions for the model, but it does not automatically resize images to those dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding = same\n",
    "\n",
    "Enhanced Edge Learning: In the context of cat images, distinguishing features such as the tips of ears, whiskers, or patterns on the fur near the edges could be crucial for accurate classification. Padding ensures that convolutions applied to the edges of an image don't shrink the spatial dimension, thus better preserving the information at the edges.\n",
    "\n",
    "Consider a scenario where your images of cats vary slightly in where the cats are positioned within the image frame. Without appropriate padding, important features at the edges might get progressively omitted in deeper layers, potentially leading to poorer model performance, especially if those edge features are critical for distinguishing between similar cat breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the `tf.keras.optimizers.Adam` optimizer and `tf.keras.losses.SparseCategoricalCrossentropy` loss function. To view training and validation accuracy for each training epoch, pass the `metrics` argument to `Model.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set from_logits=True if the model does not include a softmax activation\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `from_logits=True` the last layer should not apply softmax (it outputs raw scores)\n",
    "When using `from_logits=False`, the last layer should apply softmax to output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total overfit! The plots show that training accuracy and validation accuracy are off by large margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your test images\n",
    "directory = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test'\n",
    "\n",
    "# Load class names\n",
    "class_names = ['h', 'r', 's']  # This should be the same as `train_ds.class_names`\n",
    "\n",
    "# Iterate over each image in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):  # check for image files\n",
    "        # Load the image\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = Image.open(img_path)#.convert('RGB')\n",
    "\n",
    "        # Convert image to array (no need to normalize)\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # model expects batch of images\n",
    "\n",
    "        # Predict the class\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions, axis=1)  # get the class index\n",
    "\n",
    "        # Print the filename and predicted class name\n",
    "        print(f\"{filename}: Class {class_names[predicted_class[0]]}\")\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not accurate at all. The class r is never assigned. This could be because the model is overfitting and because the class r is underrepresented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CategoricalAccuracy` vs `SparseCategoricalAccuracy`:\n",
    "\n",
    "If labels are integer-encoded (like 0, 1, 2, 3 for four classes), use `SparseCategoricalAccuracy` and `SparseCategoricalCrossentropy`.\n",
    "If labels are one-hot encoded (like [1, 0, 0, 0] for class 0), then use `CategoricalAccuracy` and `CategoricalCrossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),  # Dropout after pooling\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "  layers.Dropout(0.5),  # Higher dropout before the final layers\n",
    "  layers.Dense(num_classes, activation='softmax') # Using softmax for probability output\n",
    "])\n",
    "\n",
    "'''\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255,input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes, name=\"outputs\", activation='softmax')\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer: Takes in your images, which should be formatted to the expected dimensions (height x width x channels). The input images are also normalized or scaled, often to a range between 0 and 1.\n",
    "\n",
    "Convolutional Layers (Conv2D): These layers apply filters to the input images to create feature maps. They extract important features from the images, such as edges, textures, or more complex patterns in deeper layers.\n",
    "\n",
    "Pooling Layers (MaxPooling2D): Reduce the spatial dimensions (width, height) of the input volume for the next convolutional layer. They help make the detection of features somewhat invariant to scale and orientation changes.\n",
    "\n",
    "Dropout Layers: Randomly drop units (and their connections) during training time to prevent overfitting by providing a form of regularization.\n",
    "\n",
    "Flatten Layer: Converts the 2D feature maps into a 1D feature vector. Necessary to transition from the convolutional layers to fully connected layers.\n",
    "\n",
    "Dense Layers: Fully connected layers that learn non-linear combinations of the high-level features extracted by the convolutional layers.\n",
    "\n",
    "Output Layer: A dense layer with a softmax activation function that outputs the probability distribution across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "  metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(171) # adjust this based on the number of epochs trained for (to avoid value error)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your test images\n",
    "directory = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test'\n",
    "\n",
    "# Load class names\n",
    "class_names = ['h', 'r', 's']  # This should be the same as `train_ds.class_names`\n",
    "\n",
    "# Iterate over each image in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):  # check for image files\n",
    "        # Load the image\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = Image.open(img_path)#.convert('RGB')\n",
    "\n",
    "        # Resize the image to match the input size expected by the model\n",
    "        img = img.resize((img_width, img_height))\n",
    "\n",
    "        # Convert image to array (no need to normalize)\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # model expects batch of images\n",
    "\n",
    "        # Predict the class\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions, axis=1)  # get the class index\n",
    "\n",
    "        # Print the filename and predicted class name\n",
    "        print(f\"{filename}: Class {class_names[predicted_class[0]]}\")\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is already better, as the r class is considered at least. Accuracy is not that good, though. Together with inspecting the low training accuracy compared to the higher validation accuracy, I think that the model is underfitting. Let's add some more layers to the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "  metrics='accuracy')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(68) # adjust this based on the number of epochs trained for (to avoid value error)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your test images\n",
    "directory = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test'\n",
    "\n",
    "# Load class names\n",
    "class_names = ['h', 'r', 's']  # This should be the same as `train_ds.class_names`\n",
    "\n",
    "# Iterate over each image in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):  # check for image files\n",
    "        # Load the image\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = Image.open(img_path)#.convert('RGB')\n",
    "\n",
    "        # Resize the image to match the input size expected by the model\n",
    "        img = img.resize((img_width, img_height))\n",
    "\n",
    "        # Convert image to array (no need to normalize)\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # model expects batch of images\n",
    "\n",
    "        # Predict the class\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions, axis=1)  # get the class index\n",
    "\n",
    "        # Print the filename and predicted class name\n",
    "        print(f\"{filename}: Class {class_names[predicted_class[0]]}\")\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results hare still not overwhelmingly accurate. The training and validation accuracy curves do not look bad, though. It seems that the underrepresented categorey still makes problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# filter size and pool_size are set to 3x3 and 2x2 respectively (may be the default anyway)\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "  metrics='accuracy')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over 4min training time with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(68) # adjust this based on the number of epochs trained for (to avoid value error)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your test images\n",
    "directory = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test'\n",
    "\n",
    "# Load class names\n",
    "class_names = ['h', 'r', 's']  # This should be the same as `train_ds.class_names`\n",
    "\n",
    "# Iterate over each image in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):  # check for image files\n",
    "        # Load the image\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = Image.open(img_path)#.convert('RGB')\n",
    "\n",
    "        # Resize the image to match the input size expected by the model\n",
    "        img = img.resize((img_width, img_height))\n",
    "\n",
    "        # Convert image to array (no need to normalize)\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # model expects batch of images\n",
    "\n",
    "        # Predict the class\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions, axis=1)  # get the class index\n",
    "\n",
    "        # Print the filename and predicted class name\n",
    "        print(f\"{filename}: Class {class_names[predicted_class[0]]}\")\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this is the best result. Simba once got classified as Hali and the only other errors we get are related to class r, which is the underrepresented class. Seems like we don't get around the data augmentation part..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from PIL import Image # for opening images\n",
    "import numpy as np # for numerical operations\n",
    "\n",
    "#load model \"models/best_model_hrs3.h5\"\n",
    "model = keras.models.load_model(\"C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\models\\\\best_model_hrs3.h5\")\n",
    "\n",
    "# define image size\n",
    "img_height = 240\n",
    "img_width = 320\n",
    "\n",
    "img = Image.open(\"C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test\\\\approach_Hali2.jpg\")\n",
    "img = img.resize((img_width, img_height))\n",
    "\n",
    "# Convert image to array (no need to normalize)\n",
    "img_array = np.array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # model expects batch of images\n",
    "\n",
    "# Predict the class\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)  # get the class index\n",
    "\n",
    "class_names = ['h', 'r', 's']\n",
    "#print(f\"Class {class_names[predicted_class[0]]}\")\n",
    "cat = class_names[predicted_class[0]]\n",
    "if cat == 'h':\n",
    "    print(\"Hali\")\n",
    "elif cat == 'r':\n",
    "    print(\"Rexxu\")\n",
    "elif cat == 's':\n",
    "    print(\"Simbi\")\n",
    "else:\n",
    "    print(\"Unbekannte Katze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline for inference with tflite\n",
    "1. Loading the model: Instead of `keras.models.load_model`, use `tf.lite.Interpreter`to load the TensorFlow Lite model \n",
    "\n",
    "2. Input and Output Handling: Allocate tensors and set the input and output tensors for the TensoFlow Lite Interpreter (no need to manually allocate tensors in Keras)\n",
    "\n",
    "3. Model Inference: Run the inference using the interpreter instead of calling `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkout the tensorflow lite model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\models\\\\best_model_hrs3.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "def print_model_details(interpreter):\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    for idx, tensor in enumerate(tensor_details):\n",
    "        print(f\"Layer {idx}: {tensor['name']}\")\n",
    "        \n",
    "# Print model details\n",
    "print_model_details(interpreter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "tflite_model_path = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\models\\\\best_model_hrs3.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)\n",
    "print(input_details[0]['shape'])\n",
    "\n",
    "# Define image size\n",
    "img_height = 240\n",
    "img_width = 320\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = \"C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test\\\\noapproach_Rex.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img = img.resize((img_width, img_height))\n",
    "img_array = np.array(img, dtype=np.float32)  # Make sure the dtype is float32\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Model expects a batch of images\n",
    "\n",
    "# Set the tensor to the image\n",
    "interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the prediction\n",
    "predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = np.argmax(predictions, axis=1)[0] # Get the class index\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "# Map the prediction to class names\n",
    "class_names = ['h', 'r', 's']\n",
    "cat = class_names[predicted_class]\n",
    "if cat == 'h':\n",
    "    print(\"Hali\")\n",
    "elif cat == 'r':\n",
    "    print(\"Rexxu\")\n",
    "elif cat == 's':\n",
    "    print(\"Simbi\")\n",
    "else:\n",
    "    print(\"Unbekannte Katze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the TFLite model\n",
    "tflite_model_path = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\models\\\\best_model_hrs3.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Define image size\n",
    "img_height = 240\n",
    "img_width = 320\n",
    "\n",
    "# Class names mapping\n",
    "class_names = ['h', 'r', 's']\n",
    "\n",
    "# Directory containing images\n",
    "image_folder = 'C:\\\\Users\\\\mirela\\\\Documents\\\\gitRepos\\\\CatWatcher\\\\examples\\\\test_training'\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Process only .jpg and .png files\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((img_width, img_height))\n",
    "        img_array = np.array(img, dtype=np.float32)  # Make sure the dtype is float32\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Model expects a batch of images\n",
    "\n",
    "        # Set the tensor to the image\n",
    "        interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the prediction\n",
    "        predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted_class = np.argmax(predictions, axis=1)[0]  # Get the class index\n",
    "        print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "        # Map the prediction to class names\n",
    "        cat = class_names[predicted_class]\n",
    "        if cat == 'h':\n",
    "            print(\"Hali\")\n",
    "        elif cat == 'r':\n",
    "            print(\"Rexxu\")\n",
    "        elif cat == 's':\n",
    "            print(\"Simbi\")\n",
    "        else:\n",
    "            print(\"Unbekannte Katze\")\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desperate Data Augmentation\n",
    "Overfitting generally occurs when there are a small number of training examples. [Data augmentation](./data_augmentation.ipynb) takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "You will implement data augmentation using the following Keras preprocessing layers: `tf.keras.layers.RandomFlip`, `tf.keras.layers.RandomRotation`, and `tf.keras.layers.RandomZoom`. These can be included inside your model like other layers, and run on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images, training=True)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255), # input shape already defined in data_augmentation layer\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "  metrics='accuracy')\n",
    "# try from_logits=True if the model does not include an activation function in the last layer\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(39) # adjust this based on the number of epochs trained for (to avoid value error)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
